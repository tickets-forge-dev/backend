<?xml version="1.0" encoding="UTF-8"?>
<story-context>
  <metadata>
    <story-id>9-4</story-id>
    <epic-id>9</epic-id>
    <title>Tech-Spec Generator (BMAD-Style Specification)</title>
    <status>ready-for-dev</status>
    <generated-date>2026-02-05</generated-date>
  </metadata>

  <story-summary>
    <description>Service that generates BMAD-style technical specifications with full codebase context. Tickets are definitive, specific, and code-aware with zero ambiguity.</description>
    <user-story>
      <role>backend developer</role>
      <capability>a service that generates BMAD-style tech specs with full codebase context</capability>
      <benefit>tickets are definitive, specific, and code-aware with zero ambiguity</benefit>
    </user-story>
  </story-summary>

  <dependencies>
    <dependency type="required">
      <id>9-1</id>
      <name>GitHub File Service</name>
      <status>review</status>
      <purpose>Read relevant code files from repository for context injection</purpose>
    </dependency>
    <dependency type="required">
      <id>9-2</id>
      <name>Project Stack Detector</name>
      <status>review</status>
      <purpose>Provide ProjectStack value object with framework, language, and tooling context</purpose>
    </dependency>
    <dependency type="required">
      <id>9-3</id>
      <name>Codebase Analyzer</name>
      <status>review</status>
      <purpose>Provide CodebaseAnalysis with detected patterns, naming conventions, and architecture</purpose>
    </dependency>
  </dependencies>

  <acceptance-criteria>
    <ac>
      <id>AC-1</id>
      <statement>Service accepts input: title, description (optional), github context (owner, repo, branch), stack info, codebase analysis</statement>
      <implementation-notes>
        - TechSpecInput interface with required/optional fields
        - Validate title is non-empty string
        - Accept FileTree and Map&lt;string, string&gt; for github context
        - Accept ProjectStack from 9-2 and CodebaseAnalysis from 9-3
      </implementation-notes>
      <test-strategy>Unit test with various input combinations</test-strategy>
    </ac>
    <ac>
      <id>AC-2</id>
      <statement>Generates Problem Statement section (what problem are we solving, why does it matter, context)</statement>
      <implementation-notes>
        - ProblemStatement interface with: narrative, whyItMatters, context, assumptions, constraints
        - LLM prompt: "Analyze title/description and context. Generate problem statement explaining what problem we're solving, why it matters, and relevant context"
        - Include framework, language, detected patterns in context
        - Extract 2-3 key assumptions and 2-3 constraints from description
      </implementation-notes>
      <test-strategy>Mock LLM response, verify structure and content presence</test-strategy>
    </ac>
    <ac>
      <id>AC-3</id>
      <statement>Generates Solution section (specific implementation steps, file paths, line numbers, code references)</statement>
      <implementation-notes>
        - SolutionSection interface with: overview, steps[], fileChanges{create, modify, delete}
        - SolutionStep includes: order, description, file (path), lineNumbers [start, end], codeSnippet
        - LLM prompt: "Generate step-by-step solution. Reference specific files by path. Include line numbers for modifications."
        - Use detected directory structure to suggest realistic file paths
        - For each step mentioning files, include full path starting from project root
        - Verify file paths exist in the provided FileTree (or are valid for creation)
      </implementation-notes>
      <test-strategy>Mock LLM response with file references, verify paths are valid or creatable</test-strategy>
    </ac>
    <ac>
      <id>AC-4</id>
      <statement>Generates In-Scope and Out-of-Scope sections (explicit boundaries)</statement>
      <implementation-notes>
        - Arrays: inScope (string[]), outOfScope (string[])
        - LLM prompt: "Based on the title and description, define what IS in scope and what IS NOT in scope"
        - Generate 3-5 items for each (at minimum)
        - Items should be specific and actionable (not vague like "performance")
        - Examples: "Create Auth.tsx component (in-scope)", "Implement OAuth providers (out-of-scope)"
      </implementation-notes>
      <test-strategy>Mock LLM response with scope lists, verify each item is specific</test-strategy>
    </ac>
    <ac>
      <id>AC-5</id>
      <statement>Generates Acceptance Criteria in BDD-style Given/When/Then format (5+ criteria)</statement>
      <implementation-notes>
        - AcceptanceCriterion interface with: given, when, then, implementationNotes
        - LLM prompt: "Generate minimum 5 acceptance criteria in Given/When/Then format. Each must be testable."
        - Format examples:
          - Given: "User is on login page"
          - When: "User enters valid email and password"
          - Then: "User is authenticated and redirected to dashboard"
        - Include edge cases (invalid input, errors, etc.)
        - Reference specific files/components where applicable in implementationNotes
      </implementation-notes>
      <test-strategy>Mock LLM response, verify count ≥ 5, verify all have given/when/then</test-strategy>
    </ac>
    <ac>
      <id>AC-6</id>
      <statement>Generates clarification questions as structured form fields (radio buttons, checkboxes, text inputs)</statement>
      <implementation-notes>
        - ClarificationQuestion interface with: id, question, type, options, defaultValue, context, impact
        - QuestionType enum: 'radio' | 'checkbox' | 'text' | 'select' | 'multiline'
        - LLM prompt: "Identify ambiguities in the request. Generate clarification questions with specific types."
        - Generate 2-4 questions to resolve ambiguities (or empty array if none)
        - Examples:
          - type: 'radio', options: ['JWT', 'Session cookies', 'OAuth']
          - type: 'checkbox', options: ['Email', 'Phone', 'Social login']
          - type: 'text', question: "What is the max password length?"
        - Each question should explain why we're asking and how answer affects spec
      </implementation-notes>
      <test-strategy>Mock LLM response with various question types, verify options match types</test-strategy>
    </ac>
    <ac>
      <id>AC-7</id>
      <statement>Lists specific files to create/modify with exact paths, line numbers, and suggested changes</statement>
      <implementation-notes>
        - FileChange interface with: path, action, lineNumbers, suggestedContent, suggestedChanges, imports, pattern
        - action: 'create' | 'modify' | 'delete'
        - For create: provide suggestedContent (stub or template)
        - For modify: provide lineNumbers [start, end] and suggestedChanges
        - imports.add: array of import statements to add
        - imports.remove: array of imports to remove
        - pattern: reference to existing pattern to follow (e.g., "Follow UserStore pattern in src/stores/UserStore.ts")
        - Validate paths using detected directory structure
        - Ensure line numbers are reasonable (don't extend beyond file)
      </implementation-notes>
      <test-strategy>Mock LLM response with file changes, verify paths are valid and actions are clear</test-strategy>
    </ac>
    <ac>
      <id>AC-8</id>
      <statement>Output has ZERO ambiguity (no "or" statements, no "might", all decisions definitive)</statement>
      <implementation-notes>
        - Ambiguity detection function scanning for: "or", "might", "could", "possibly", "maybe", "optional", "perhaps"
        - After LLM generation, scan entire spec for these words
        - If found, use LLM again to rewrite with definitive language
        - Replacements: "might" → "will", "could" → "must", "or" → explicit option selection
        - Store ambiguityFlags array with issues found
        - Final spec must have empty ambiguityFlags array OR minor warnings only
      </implementation-notes>
      <test-strategy>Generate spec, verify no ambiguity markers in output, test rewriting function</test-strategy>
    </ac>
    <ac>
      <id>AC-9</id>
      <statement>Output includes quality score (0-100) indicating completeness and specificity</statement>
      <implementation-notes>
        - qualityScore: number (0-100)
        - Scoring algorithm:
          - Problem Statement completeness (0-20): presence of all fields, length, clarity signals
          - Solution specificity (0-30): file paths present, line numbers provided, code snippets, step count
          - Acceptance Criteria quality (0-20): count (5+ = full), testability, edge cases
          - File Changes clarity (0-15): paths are specific, line numbers present, imports clear
          - Ambiguity score (0-15): deduct 1 per ambiguity marker, reward for definitive language
        - Final score = sum of all categories
        - Edge cases: if problem statement empty, max 20; if no file changes, max 85
      </implementation-notes>
      <test-strategy>Test scoring with various spec completeness levels, verify score range 0-100</test-strategy>
    </ac>
    <ac>
      <id>AC-10</id>
      <statement>Comprehensive unit tests with mocked LLM (100% coverage)</statement>
      <implementation-notes>
        - Use Jest for testing
        - Mock LLM client with deterministic responses
        - Test each method independently: generateProblemStatement, generateSolution, etc.
        - Test with different project types (Next.js, React, Python, Go)
        - Test error scenarios: LLM timeout, API failure, malformed response
        - Test edge cases: minimal input, very complex request
        - All code paths must be covered
        - Performance test: verify entire generation &lt; 15 seconds (with mocked LLM)
      </implementation-notes>
      <test-strategy>Comprehensive test suite with 40+ test cases, mocked LLM</test-strategy>
    </ac>
    <ac>
      <id>AC-11</id>
      <statement>JSDoc documentation with prompt engineering notes</statement>
      <implementation-notes>
        - Document each method with @param, @returns, @example
        - Include prompt engineering strategy: context injection, role setting, rules
        - Document the scoring algorithm with detailed breakdown
        - Document ambiguity detection rules
        - Provide example system prompt and component prompts
        - Document how to customize prompts for different project types
      </implementation-notes>
      <test-strategy>Code review verification of documentation</test-strategy>
    </ac>
  </acceptance-criteria>

  <test-ideas>
    <category name="Input Validation">
      <test>Should accept valid TechSpecInput with all required fields</test>
      <test>Should accept valid TechSpecInput with partial optional fields</test>
      <test>Should reject empty title</test>
      <test>Should handle missing description gracefully</test>
      <test>Should validate FileTree and files map provided</test>
    </category>

    <category name="Problem Statement Generation">
      <test>Should generate problem narrative from title and description</test>
      <test>Should include framework context in problem statement</test>
      <test>Should extract 2-3 assumptions from description</test>
      <test>Should extract 2-3 constraints from description</test>
      <test>Should include project patterns in context</test>
    </category>

    <category name="Solution Generation">
      <test>Should generate solution overview</test>
      <test>Should include 5+ solution steps</test>
      <test>Should reference existing files with correct paths</test>
      <test>Should include line numbers for file modifications</test>
      <test>Should suggest new files with realistic paths based on architecture</test>
      <test>Should follow project's naming conventions in file suggestions</test>
      <test>Should reference existing code patterns</test>
    </category>

    <category name="Acceptance Criteria">
      <test>Should generate minimum 5 acceptance criteria</test>
      <test>Should format each as Given/When/Then</test>
      <test>Should reference specific files in implementation notes</test>
      <test>Should include edge case scenarios</test>
      <test>Should make criteria testable and unambiguous</test>
      <test>Should align criteria with problem statement</test>
    </category>

    <category name="Scope Boundaries">
      <test>Should generate in-scope items (3-5 minimum)</test>
      <test>Should generate out-of-scope items (3-5 minimum)</test>
      <test>Should make scope items specific and actionable</test>
      <test>Should not overlap between in-scope and out-of-scope</test>
    </category>

    <category name="Clarification Questions">
      <test>Should generate 0-4 clarification questions for ambiguous requests</test>
      <test>Should use correct question types: radio, checkbox, text, select</test>
      <test>Should provide reasonable default values</test>
      <test>Should include context explaining why question is asked</test>
      <test>Should include impact field explaining how answer affects spec</test>
    </category>

    <category name="File Changes">
      <test>Should identify files to create from solution</test>
      <test>Should identify files to modify from solution</test>
      <test>Should include line numbers for modifications</test>
      <test>Should suggest imports to add/remove</test>
      <test>Should reference existing pattern to follow</test>
      <test>Should validate file paths against project structure</test>
    </category>

    <category name="Ambiguity Detection">
      <test>Should detect "or" statements in spec</test>
      <test>Should detect "might" and "could" language</test>
      <test>Should detect "possibly" and "maybe"</test>
      <test>Should flag uncertain language and rewrite to definitive</test>
      <test>Should store ambiguityFlags array with issues</test>
      <test>Should have empty ambiguityFlags in final output</test>
    </category>

    <category name="Quality Scoring">
      <test>Should award points for problem statement completeness (0-20)</test>
      <test>Should award points for solution specificity (0-30)</test>
      <test>Should award points for AC quality (0-20)</test>
      <test>Should award points for file changes clarity (0-15)</test>
      <test>Should deduct points for ambiguity (0-15)</test>
      <test>Should produce consistent scores for same inputs</test>
      <test>Should score incomplete specs lower (no problem statement = max 20)</test>
    </category>

    <category name="Project Type Handling">
      <test>Should generate Next.js-appropriate file paths and imports</test>
      <test>Should generate React-appropriate component suggestions</test>
      <test>Should generate Python-appropriate file/function suggestions</test>
      <test>Should generate Go-appropriate file/package suggestions</test>
      <test>Should use correct naming conventions per project type</test>
    </category>

    <category name="Error Handling">
      <test>Should handle LLM timeout (30 second max)</test>
      <test>Should handle LLM API failure with retry logic</test>
      <test>Should handle malformed LLM response (parse error)</test>
      <test>Should handle empty LLM response</test>
      <test>Should provide meaningful error messages</test>
    </category>

    <category name="Edge Cases">
      <test>Should handle minimal input (title only)</test>
      <test>Should handle very long description (5000+ chars)</test>
      <test>Should handle complex multi-layer requests</test>
      <test>Should handle requests for deprecated technologies</test>
      <test>Should handle requests for very large features</test>
      <test>Should handle requests with security implications</test>
    </category>

    <category name="Integration">
      <test>Should work with Story 9-1 (GitHub File Service) output</test>
      <test>Should work with Story 9-2 (Project Stack Detector) output</test>
      <test>Should work with Story 9-3 (Codebase Analyzer) output</test>
      <test>Should generate complete TechSpec ready for Story 9-5</test>
    </category>
  </test-ideas>

  <architecture-guidelines>
    <layer>Domain (pattern-analysis/TechSpecGenerator.ts)</layer>
    <content>
      - TechSpecInput interface
      - ProblemStatement interface
      - SolutionStep and SolutionSection interfaces
      - AcceptanceCriterion interface
      - ClarificationQuestion interface with QuestionType
      - FileChange interface with action types
      - TechSpec composite interface
      - TechSpecGenerator service interface (7 methods)
    </content>
  </architecture-guidelines>

  <architecture-guidelines>
    <layer>Application (services/TechSpecGeneratorImpl.ts)</layer>
    <content>
      - TechSpecGeneratorImpl class implementing TechSpecGenerator
      - LLM client integration (Anthropic API)
      - Prompt templates for each section generation
      - Context injection system for stack/patterns/conventions
      - Ambiguity detection and removal
      - Quality scoring algorithm
      - Validation and error handling
      - File path validation against detected structure
    </content>
  </architecture-guidelines>

  <prompt-engineering>
    <system-prompt-template>
      You are a technical specification writer specializing in code-aware specifications.
      Your task is to generate ZERO-AMBIGUITY technical specifications.

      RULES:
      1. No "or" statements - all decisions are definitive
      2. No "might", "could", "possibly" - use "will", "must", "shall"
      3. All file paths must be complete and specific to the project
      4. All code references must include exact line numbers
      5. All acceptance criteria must be testable and unambiguous
      6. Always reference existing code patterns and conventions

      PROJECT CONTEXT:
      - Framework: {FRAMEWORK}
      - Language: {LANGUAGE}
      - Architecture: {ARCHITECTURE}
      - Testing Strategy: {TESTING_STRATEGY}
      - Naming Conventions: {NAMING_CONVENTIONS}

      Follow these conventions in your output:
      {CONVENTIONS}

      Reference these code patterns:
      {CODE_SAMPLES}
    </system-prompt-template>

    <component-prompts>
      <prompt name="problem-statement">
        Analyze the following request and generate a problem statement:

        Title: {TITLE}
        Description: {DESCRIPTION}

        Generate a JSON object with:
        {
          "narrative": "Clear explanation of what problem we're solving",
          "whyItMatters": "Impact and importance",
          "context": "Relevant background information",
          "assumptions": ["assumption 1", "assumption 2", "assumption 3"],
          "constraints": ["constraint 1", "constraint 2", "constraint 3"]
        }
      </prompt>

      <prompt name="solution">
        Based on the following problem and project context, generate a detailed solution:

        Problem: {PROBLEM_STATEMENT}
        Project: {FRAMEWORK} {LANGUAGE}
        Architecture: {ARCHITECTURE}

        Generate a JSON object with:
        {
          "overview": "High-level solution description",
          "steps": [
            {
              "order": 1,
              "description": "Step description",
              "file": "path/to/file.ts",
              "lineNumbers": [10, 20],
              "codeSnippet": "relevant code"
            }
          ],
          "fileChanges": {
            "create": ["path/to/new/file.ts"],
            "modify": ["path/to/existing/file.ts"],
            "delete": []
          }
        }

        Use actual file paths that match the project structure.
        Include line numbers for all modifications.
      </prompt>

      <prompt name="acceptance-criteria">
        Generate 5+ acceptance criteria in Given/When/Then format for:

        Problem: {PROBLEM_STATEMENT}
        Solution: {SOLUTION_OVERVIEW}

        Return JSON array:
        [
          {
            "given": "Initial condition",
            "when": "Action taken",
            "then": "Expected result",
            "implementationNotes": "How to test this"
          }
        ]

        Make each criterion testable and unambiguous.
      </prompt>

      <prompt name="clarification">
        Identify ambiguities in this request that need clarification:

        Original Request: {TITLE} - {DESCRIPTION}

        Generate JSON array of clarification questions:
        [
          {
            "id": "q1",
            "question": "Specific question",
            "type": "radio|checkbox|text|select|multiline",
            "options": ["option 1", "option 2"],
            "defaultValue": "option 1",
            "context": "Why we're asking",
            "impact": "How answer affects spec"
          }
        ]

        Generate only if ambiguities exist.
      </prompt>

      <prompt name="file-changes">
        Based on the solution, identify all files to create/modify:

        Solution: {SOLUTION_TEXT}
        Project Structure: {DIRECTORY_STRUCTURE}

        Return JSON array:
        [
          {
            "path": "src/components/Auth.tsx",
            "action": "create|modify|delete",
            "lineNumbers": [10, 50],
            "suggestedContent": "code for creation",
            "suggestedChanges": "code changes for modification",
            "imports": {
              "add": ["import { useState } from 'react'"],
              "remove": ["import { oldHook }"]
            },
            "pattern": "Reference to existing pattern"
          }
        ]
      </prompt>
    </component-prompts>

    <context-injection-strategy>
      <priority>1</priority>
      <item>Stack (framework, language, tools) - always include</item>
      <item>Architecture pattern (feature-based, layered, etc.) - always include</item>
      <item>Naming conventions (camelCase, PascalCase) - always include</item>
      <item>Detected patterns (state management, testing strategy) - always include</item>
      <item>Top 5 code samples (existing patterns to follow) - include if space allows</item>
      <item>Directory structure (key directories) - include if space allows</item>
      <item>Existing file references - include for modifications</item>
    </context-injection-strategy>

    <token-limit-strategy>
      - System prompt: ~400 tokens
      - Context injection: ~1000 tokens (framework, patterns, samples)
      - Component prompt: ~300 tokens
      - User request: ~200 tokens
      - Total budget: ~2000 tokens per request
      - If exceeding limit: prioritize stack/architecture/patterns; trim samples
    </token-limit-strategy>
  </prompt-engineering>

  <quality-scoring-algorithm>
    <category name="Problem Statement Completeness" max="20">
      <criterion name="narrative_present">Narrative field exists and has content (0-7 points)</criterion>
      <criterion name="why_it_matters">whyItMatters field exists and specific (0-7 points)</criterion>
      <criterion name="context_provided">context field exists and relevant (0-3 points)</criterion>
      <criterion name="assumptions">assumptions array has 2+ items (0-2 points)</criterion>
      <criterion name="constraints">constraints array has 2+ items (0-1 points)</criterion>
    </category>

    <category name="Solution Specificity" max="30">
      <criterion name="steps_count">5+ steps present (0-8 points)</criterion>
      <criterion name="file_paths">All steps referencing files have full paths (0-8 points)</criterion>
      <criterion name="line_numbers">Modifications include line numbers (0-8 points)</criterion>
      <criterion name="code_snippets">Code references included where applicable (0-3 points)</criterion>
      <criterion name="file_changes_completeness">create/modify/delete clearly listed (0-3 points)</criterion>
    </category>

    <category name="Acceptance Criteria Quality" max="20">
      <criterion name="minimum_count">5+ criteria present (0-7 points)</criterion>
      <criterion name="bdd_format">All follow Given/When/Then format (0-7 points)</criterion>
      <criterion name="testability">Criteria are testable and measurable (0-3 points)</criterion>
      <criterion name="edge_cases">At least 1 edge case or error scenario (0-3 points)</criterion>
    </category>

    <category name="File Changes Clarity" max="15">
      <criterion name="paths_specific">All paths are specific, not generic (0-5 points)</criterion>
      <criterion name="line_numbers_present">Modifications have line number ranges (0-5 points)</criterion>
      <criterion name="imports_clear">Import additions/removals are explicit (0-3 points)</criterion>
      <criterion name="pattern_reference">Reference to existing pattern included (0-2 points)</criterion>
    </category>

    <category name="Ambiguity and Language" max="15">
      <criterion name="no_or_statements">No "or" statements in entire spec (0-3 points)</criterion>
      <criterion name="no_uncertain_language">No "might", "could", "possibly" (0-3 points)</criterion>
      <criterion name="definitive_statements">Uses "will", "must", "shall" (0-3 points)</criterion>
      <criterion name="deduction_per_ambiguity">-1 point per ambiguity marker found (min 0)</criterion>
      <criterion name="bonus_definitive_language">+2 bonus for high confidence statements</criterion>
    </category>

    <scoring-edge-cases>
      <case>Empty problem statement: maximum score 20 (rest 0)</case>
      <case>No acceptance criteria: maximum score 50 (missing 20 points)</case>
      <case>No file changes: maximum score 85 (missing 15 points)</case>
      <case>Multiple critical ambiguities: maximum score 75 (ambiguity section capped at 5/15)</case>
    </scoring-edge-cases>
  </quality-scoring-algorithm>

  <implementation-checklist>
    <item status="pending">Create TechSpecGenerator domain interface (560 lines)</item>
    <item status="pending">Implement TechSpecGeneratorImpl service (650 lines)</item>
    <item status="pending">Create comprehensive prompt templates</item>
    <item status="pending">Implement LLM integration with Anthropic API</item>
    <item status="pending">Implement context injection system</item>
    <item status="pending">Implement ambiguity detection and rewriting</item>
    <item status="pending">Implement quality scoring algorithm</item>
    <item status="pending">Create test file with 40+ test cases</item>
    <item status="pending">Add comprehensive JSDoc documentation</item>
    <item status="pending">Verify all AC criteria met</item>
    <item status="pending">Run tests: verify 20/20 passing with 100% coverage</item>
    <item status="pending">Update sprint status to review</item>
  </implementation-checklist>

  <context-available>
    <item>Story 9-1 context available: GitHubFileService fully implemented ✓</item>
    <item>Story 9-2 context available: ProjectStackDetector fully implemented ✓</item>
    <item>Story 9-3 context available: CodebaseAnalyzer fully implemented ✓</item>
    <item>All dependencies complete and ready to integrate</item>
  </context-available>

  <notes>
    <note type="prompt-engineering">
      The key to quality specs is context injection at every level. Include:
      - Project stack (framework, language, tools)
      - Detected architecture and patterns
      - Naming conventions and code samples
      - File structure to suggest realistic paths
      This prevents generic, unusable specs.
    </note>

    <note type="ambiguity">
      ZERO ambiguity is non-negotiable. After each section generation:
      1. Scan for "or", "might", "could", "possibly", "maybe"
      2. Use LLM to rewrite with definitive statements
      3. Repeat until no markers found
      4. Store any issues in ambiguityFlags array
    </note>

    <note type="quality-scoring">
      Quality score must be reliable and consistent. The algorithm is deterministic:
      each section has clear point allocations. Test scoring with various completeness
      levels to verify consistency and calibration.
    </note>

    <note type="llm-integration">
      Use streaming for large responses. Set 30-second timeout per LLM call.
      Implement exponential backoff retry (3 attempts) for transient failures.
      Always inject project context to prevent generic outputs.
    </note>

    <note type="testing">
      Mock LLM responses completely for unit tests. Create fixture responses
      for each component (problem statement, solution, AC, questions, file changes).
      Only do real LLM integration tests manually (not in CI).
    </note>
  </notes>

  <status>ready-for-dev</status>
  <date-ready>2026-02-05</date-ready>
  <blockers>None - all dependencies complete</blockers>
</story-context>
